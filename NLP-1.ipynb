{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standardize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our deeds are the reason of this  earthquake m...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heard about  earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forest fire near la ronge sask  canada</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text choose_one  class_label\n",
       "0                 just happened a terrible car crash   Relevant            1\n",
       "1  our deeds are the reason of this  earthquake m...   Relevant            1\n",
       "2  heard about  earthquake is different cities, s...   Relevant            1\n",
       "3  there is a forest fire at spot pond, geese are...   Relevant            1\n",
       "4             forest fire near la ronge sask  canada   Relevant            1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = pd.read_csv('social_media_clean_text.csv')\n",
    "input_file.columns=['text', 'choose_one', 'class_label']\n",
    "\n",
    "def standardize_text(df, text_field):\n",
    "    # normalize by turning all letters into lowercase\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    # get rid of URLS\n",
    "    df[text_field] = df[text_field].apply(lambda i: re.sub(r\"http\\S+\", \"\", i))  \n",
    "    return df\n",
    "\n",
    "clean_questions = standardize_text(input_file, \"text\")\n",
    "clean_questions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>class_label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, happened, a, terrible, car, crash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our deeds are the reason of this  earthquake m...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heard about  earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>[heard, about, earthquake, is, different, citi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>[there, is, a, forest, fire, at, spot, pond, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forest fire near la ronge sask  canada</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text choose_one  class_label  \\\n",
       "0                 just happened a terrible car crash   Relevant            1   \n",
       "1  our deeds are the reason of this  earthquake m...   Relevant            1   \n",
       "2  heard about  earthquake is different cities, s...   Relevant            1   \n",
       "3  there is a forest fire at spot pond, geese are...   Relevant            1   \n",
       "4             forest fire near la ronge sask  canada   Relevant            1   \n",
       "\n",
       "                                              tokens  \n",
       "0          [just, happened, a, terrible, car, crash]  \n",
       "1  [our, deeds, are, the, reason, of, this, earth...  \n",
       "2  [heard, about, earthquake, is, different, citi...  \n",
       "3  [there, is, a, forest, fire, at, spot, pond, g...  \n",
       "4      [forest, fire, near, la, ronge, sask, canada]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "clean_questions[\"tokens\"] = clean_questions[\"text\"].apply(tokenizer.tokenize)\n",
    "# Note: yahan ham simple <clean_questions['tokens'] = [i.split() for i in clean_questions.text]> bhi kar sakty \n",
    "# thy lekin us me kuch galtyan aati hen qomans or quotation mark wagera ki.. is data par ye simple code or jo\n",
    "# code hamara asal me h in dono ko result almont 1/2 data me same h, or 1/5 me masly hen\n",
    "clean_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132260 words total, with a vocabulary size of 17070\n"
     ]
    }
   ],
   "source": [
    "## [EDA] Explore words and sentences\n",
    "all_tokens = [token for tokens in clean_questions[\"tokens\"] for token in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in clean_questions[\"tokens\"]]\n",
    "VOCAB = sorted(list(set(all_tokens)))\n",
    "\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_tokens), len(VOCAB)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit data in CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7425, 15016)\n",
      "(1857, 15016)\n"
     ]
    }
   ],
   "source": [
    "list_corpus = clean_questions[\"text\"]\n",
    "list_labels = clean_questions[\"class_label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, \n",
    "                                                    list_labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=40)\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer='word', token_pattern=r'\\w+')\n",
    "\n",
    "bow = dict()\n",
    "# total unique words in our data is 17070, magar jab ham count_vectorizer me is data ko fit \n",
    "# karen gy to wo 2054 values ko chor dy ga, jo value chor raha h un me sy kafi  typos ya \n",
    "# plural hen\n",
    "bow[\"train\"] = (count_vectorizer.fit_transform(X_train), y_train)\n",
    "bow[\"test\"]  = (count_vectorizer.transform(X_test), y_test)\n",
    "\n",
    "print(bow[\"train\"][0].shape)\n",
    "print(bow[\"test\"][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit data in TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7425, 15016)\n",
      "(1857, 15016)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\w+')\n",
    "\n",
    "tfidf = dict()\n",
    "tfidf[\"train\"] = (tfidf_vectorizer.fit_transform(X_train), y_train)\n",
    "tfidf[\"test\"]  = (tfidf_vectorizer.transform(X_test), y_test)\n",
    "\n",
    "print(tfidf[\"train\"][0].shape)\n",
    "print(tfidf[\"test\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf: \n",
      "{0.0, 0.1644577804300159, 0.15244080441349195, 0.40155831974942013, 0.22780705078113467, 0.27598493701225013, 0.17903638074793415, 0.3696967282268077, 0.23848397390927262, 0.2653465371014585, 0.08237693822208582, 0.10890150825569447, 0.23551689351944025, 0.19030608767815718, 0.12870725734996555, 0.1276342175979817, 0.15443718428522887, 0.1920674547330686, 0.18864262620124475, 0.22001515993286655, 0.13998320781004508}\n",
      "\n",
      "bow: \n",
      "{0, 1, 2, 3, 5}\n"
     ]
    }
   ],
   "source": [
    "# <tfidf> me unique values relevent to <bow> zyada hen, jahan par <tfidf> me <0> h wahan <bow> me bhi <0> h, \n",
    "# agar is k ilawa koi value h to <tfidf> me wo row accur and whole document accur this word sy nikli h\n",
    "# or <bow> me simple wo wo value h jo is  particular word ki accurence is particular obsevation me h.\n",
    "\n",
    "a = tfidf['train'][0].toarray()\n",
    "b = bow['train'][0].toarray()\n",
    "print('tfidf: ')\n",
    "print(set(list(a[154])))\n",
    "print('\\nbow: ')\n",
    "print(set(list(b[154])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_questions_tokens, generate_missing=False):\n",
    "    embeddings = clean_questions_tokens.apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "# Call the functions\n",
    "embeddings = get_word2vec_embeddings(word2vec, clean_questions['tokens'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
