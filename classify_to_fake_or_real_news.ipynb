{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copy file from another directory and rename it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = !pwd\n",
    "os.chdir('/home/amir/Desktop/learning/NLP/NLP')\n",
    "!cp fake_or_real_news.csv /home/amir/Desktop/github/NLP/\n",
    "os.chdir(current_directory[0])\n",
    "os.rename('fake_or_real_news.csv', 'labeld_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire data shape:  (6335, 4)\n",
      "train qty:  4751\n",
      "test qty:  1584\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('labeld_news.csv')\n",
    "\n",
    "# shape of data\n",
    "print('Entire data shape: ', df.shape)\n",
    "\n",
    "# response variabel\n",
    "y = df.label\n",
    "\n",
    "# split data to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y)\n",
    "\n",
    "# train / test shape\n",
    "print('train qty: ', len(y_train))\n",
    "print('test qty: ', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we use 2 ways to vectorize data\n",
    "##### CountVectorizer\n",
    "##### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 59816 features\n",
      "First 10 Features in our model: \n",
      " ['00', '000', '0000', '000000031', '00000031', '0001', '0001pt', '0002', '000billion', '000ft']\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "# CountVectorizer = token accurence in this observation \n",
    "# see https://github.com/amirsaleem1990/NLP/blob/master/CountVectorizer%20(sklearn).png\n",
    "\n",
    "# CountVectorizer object\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "# <stop_words= \"english\"> English language k stop words ko drop kar do\n",
    "\n",
    "# transform the train data\n",
    "count_train = count_vectorizer.fit_transform(X_train,y_train)\n",
    "\n",
    "# transform the test data\n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# hwo many features in our model?\n",
    "print('we have {} features'.format(len(count_vectorizer.vocabulary_)))\n",
    "\n",
    "print('First 10 Features in our model: \\n', count_vectorizer.get_feature_names()[:10])\n",
    "# in the count_train, and count_test we have no column, you can \n",
    "# identify each feature by: \n",
    "# count_vectorizer.vocabulary_\n",
    "# this command return a dictnory, where  each key is actual feature, and each value is a index of column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 59812 features\n",
      "\n",
      "First 10 Features in our model: \n",
      " ['00', '000', '0000', '000000031', '00000031', '0001', '0001pt', '0002', '000billion', '000ft']\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer\n",
    "# how its work: https://github.com/amirsaleem1990/NLP/blob/master/sklearn%20TFIDF.png\n",
    "# how its work: https://github.com/amirsaleem1990/NLP/blob/master/TF-IDF.png\n",
    "\n",
    "# tfidf_vectorizer object\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.5)\n",
    "# <max_df = 0.5> koi word agar 50% yar is sy zyada observations me repeat ho raha ho to us work ko drop kar do,\n",
    "# ye bohot comon word h.\n",
    "# <stop_words= \"english\"> English language k stop words ko drop kar do\n",
    "\n",
    "# transform the train data\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train,y_train)\n",
    "\n",
    "# transform the test data\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# hwo many features in our model?\n",
    "print('we have {} features\\n'.format(len(tfidf_vectorizer.vocabulary_)))\n",
    "\n",
    "print('First 10 Features in our model: \\n', tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# in the tfidf_train, and tfidf_test we have no column, you can \n",
    "# identify each feature by: \n",
    "# count_vectorizer.vocabulary_\n",
    "# this command return a dictnory, where  each key is actual feature, and each value is a index of column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These Features just in one dataframe:\n",
      "{'new', 'said', 'people', 'time'}\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# is Features of CountVectorizer and Features of TfidfVectorizer are the same, or there are deffrence?\n",
    "\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A,columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "print('These Features just in one dataframe:')\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Naive Bayes to Classify Fake News on Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8939393939393939\n",
      "[[658 120]\n",
      " [ 48 758]]\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes classifier object\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# predect on test data\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = metrics.accuracy_score(y_test,pred)\n",
    "print(accuracy)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test,pred,labels=['FAKE','REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Naive Bayes to Classify Fake News on Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8522727272727273\n",
      "[[557 221]\n",
      " [ 13 793]]\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes classifier object\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# training data\n",
    "nb_classifier.fit(tfidf_train,y_train)\n",
    "\n",
    "# predect on test data\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, pred)\n",
    "print(accuracy)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE','REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# effect of alpha on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8882575757575758\n",
      "\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.9027777777777778\n",
      "\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.898989898989899\n",
      "\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.8907828282828283\n",
      "\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.8813131313131313\n",
      "\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.875\n",
      "\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.8667929292929293\n",
      "\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.8611111111111112\n",
      "\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.8592171717171717\n",
      "\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.8554292929292929\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# diffrent aplha's\n",
    "alphas = np.arange(0,1,0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    nb_classifier.fit(tfidf_train,y_train)\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    score = metrics.accuracy_score(y_test,pred)\n",
    "    return score\n",
    "\n",
    "# train over diffrent alpha values, and print the scores\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Fake News Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE [(-11.388621924420084, '0000'), (-11.388621924420084, '0001'), (-11.388621924420084, '0001pt'), (-11.388621924420084, '0002'), (-11.388621924420084, '000billion'), (-11.388621924420084, '000km'), (-11.388621924420084, '004s'), (-11.388621924420084, '006s'), (-11.388621924420084, '007'), (-11.388621924420084, '007s'), (-11.388621924420084, '008s'), (-11.388621924420084, '0099'), (-11.388621924420084, '00am'), (-11.388621924420084, '00p'), (-11.388621924420084, '00pm'), (-11.388621924420084, '013c2812c9'), (-11.388621924420084, '014'), (-11.388621924420084, '018'), (-11.388621924420084, '01am'), (-11.388621924420084, '020')]\n",
      "REAL [(-7.730884958182809, 'presidential'), (-7.70946186677282, 'rubio'), (-7.700150938819911, 'democratic'), (-7.6945129012222635, 'states'), (-7.688808900222656, 'gop'), (-7.679324740060702, 'bush'), (-7.61355756825691, 'voters'), (-7.596668564101243, 'republicans'), (-7.584789476246648, 'house'), (-7.511875829951274, 'percent'), (-7.396256259160873, 'party'), (-7.35561830356615, 'cruz'), (-7.33959619319905, 'state'), (-7.298120218389388, 'republican'), (-7.296113946014585, 'campaign'), (-7.245904538195886, 'president'), (-7.152908419875719, 'sanders'), (-7.082476939014299, 'obama'), (-6.672173708850477, 'clinton'), (-6.257311372173214, 'trump')]\n"
     ]
    }
   ],
   "source": [
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
